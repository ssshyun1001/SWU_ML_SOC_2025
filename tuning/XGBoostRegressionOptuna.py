import optuna
import time
from xgboost import XGBRegressor
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

def objective(trial):
    trial_start = time.time()  # ğŸ”¸ ì´ trialì˜ ì‹œì‘ ì‹œê°„

    params = {
        "n_estimators":     trial.suggest_int("n_estimators", 200, 400),
        "learning_rate":    trial.suggest_float("learning_rate", 0.01, 0.2, log=True),
        "max_depth":        trial.suggest_int("max_depth", 6, 12),
        "min_child_weight": trial.suggest_int("min_child_weight", 1, 4),
        "subsample":        trial.suggest_float("subsample", 0.8, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.8, 1.0),
        "gamma":            trial.suggest_float("gamma", 0.0, 0.3),

        # CPU ì„¤ì •ìœ¼ë¡œ ë³€ê²½
        "tree_method":      "hist",           # CPUìš© íˆìŠ¤í† ê·¸ë¨ ë°©ë²•
        "device":           "cpu",            # ëª…ì‹œì ìœ¼ë¡œ CPU ì§€ì •
        "random_state":     42,
        "n_jobs":           -1,               # ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš©
        "eval_metric":      "rmse",
        "objective":        "reg:squarederror",
        "early_stopping_rounds": 20          # ì¡°ê¸° ì¢…ë£Œë¥¼ íŒŒë¼ë¯¸í„°ì— í¬í•¨
    }

    model = XGBRegressor(**params)

    # XGBoost ë²„ì „ì— ë”°ë¥¸ í˜¸í™˜ì„± ì²˜ë¦¬
    try:
        model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            verbose=False
        )
    except TypeError:
        # early_stopping_roundsê°€ fit()ì—ì„œ ì§€ì›ë˜ì§€ ì•ŠëŠ” ê²½ìš°
        params_without_early_stop = params.copy()
        params_without_early_stop.pop('early_stopping_rounds', None)
        model = XGBRegressor(**params_without_early_stop)
        model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            verbose=False
        )

    preds = model.predict(X_val)
    rmse = np.sqrt(mean_squared_error(y_val, preds))

    # ğŸ”¸ ì‹œê°„ ì¶œë ¥
    trial_time = time.time() - trial_start
    total_elapsed = time.time() - start
    print(f"[Trial {trial.number:2d}] RMSE: {rmse:.5f} | Trial time: {trial_time/60:.2f}ë¶„ | Total: {total_elapsed/60:.2f}ë¶„ | Params: {trial.params}")

    return rmse

# Optuna ìµœì í™” ì‹¤í–‰
start = time.time()
study = optuna.create_study(direction="minimize", study_name="xgb_cpu_tuning")
study.optimize(objective, n_trials=50, show_progress_bar=True)

print("\n" + "="*60)
print("â–¶ Best params :", study.best_params)
print(f"â–¶ Best RMSE   : {study.best_value:.5f}")

# ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ
best_params = study.best_params.copy()
best_params.update({
    "tree_method": "hist",
    "device": "cpu",
    "random_state": 42,
    "n_jobs": -1,
    "eval_metric": "rmse",
    "objective": "reg:squarederror"
})

print("\nâ–¶ ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ì „ì²´ ë°ì´í„°ì…‹ì—ì„œ ì¬í•™ìŠµ ì¤‘...")
best_model = XGBRegressor(**best_params)
best_model.fit(X_train_full, y_train_full)

# í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ í‰ê°€
y_pred = best_model.predict(X_test)

print("\n" + "="*60)
print("â–¶ ìµœì¢… í…ŒìŠ¤íŠ¸ ì„±ëŠ¥:")
print(f"â–¶ Test RMSE   : {np.sqrt(mean_squared_error(y_test, y_pred)):.5f}")
print(f"â–¶ Test RÂ²     : {r2_score(y_test, y_pred):.5f}")
print(f"â±ï¸ ì´ ê²½ê³¼ì‹œê°„: {time.time() - start:.2f}ì´ˆ")